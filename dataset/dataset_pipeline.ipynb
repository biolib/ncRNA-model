{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d850be1",
   "metadata": {},
   "source": [
    "# Dataset creation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c18099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import gzip\n",
    "import os\n",
    "import io\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm \n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b16b0",
   "metadata": {},
   "source": [
    "### Download TSA records\n",
    "First off. Download mastertable from: https://www.ncbi.nlm.nih.gov/Traces/wgs/?page=1&view=tsa\n",
    "\n",
    "Run the below to download the per-species raw TSA FASTAs. This runs on `'PRI|MAM|ROD'` but is easily extendable to other species families. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = 'PRI|MAM|ROD' # \"PRI\" (primates) or \"MAM\" (mammals) or \"ROD\" (rodents)\n",
    "tsa_master = pd.read_csv(\"tsa_lookup_mastertable.csv\")\n",
    "\n",
    "# print all unique values in the \"div_s\" column\n",
    "print(tsa_master['div_s'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define species families\n",
    "species = 'PRI|MAM|ROD' # \"PRI\" (primates) or \"MAM\" (mammals) or \"ROD\" (rodents)\n",
    "tsa_master = pd.read_csv(\"tsa_lookup_mastertable.csv\")\n",
    "os.makedirs(f\"tsa_species_fasta_output_raw_{species}\", exist_ok=True)\n",
    "tsa_raw_output_dir = f\"tsa_species_fasta_output_raw_{species}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53008d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From column \"div_s\" extract all entries with the specified species family \n",
    "filtered_df = tsa_master[tsa_master['div_s'].str.contains(species, na=False)]\n",
    "species_char_count = defaultdict(int)\n",
    "filtered_entries_count = defaultdict(int)\n",
    "\n",
    "for index, row in tqdm(filtered_df.iterrows(), total=filtered_df.shape[0], desc=\"Processing rows\"):\n",
    "    try:\n",
    "        if pd.isna(row['organism_an']):\n",
    "            print(f\"Skipping row {index} - no organism name\")\n",
    "            continue\n",
    "\n",
    "        species_name = row['organism_an']\n",
    "        filename = species_name.replace(' ', '_')\n",
    "        prefix = row[\"prefix_s\"] # get prefix\n",
    "        identifier = prefix[0] # get identifier (first letter)\n",
    "        url = f\"https://ftp.ncbi.nlm.nih.gov/genbank/tsa/{identifier}/tsa.{prefix[:-2]}.1.fsa_nt.gz\" # construct FTP URL. remove last two letters from prefix\n",
    "        tqdm.write(f\"Downloading {url} for {species_name}...\")\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            output_file = os.path.join(tsa_raw_output_dir, f\"{filename}.fasta\")\n",
    "            with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n",
    "                fasta_content = f.read().decode('utf-8')\n",
    "                entries = fasta_content.split(\">\")\n",
    "                filtered_entries = []\n",
    "                for entry in entries[1:]:\n",
    "                    # Very coarse-grained filtering: check if \"mRNA\" is in the header line (first line of the entry)\n",
    "                    header = entry.split('\\n', 1)[0]\n",
    "                    if \"mRNA\" not in header:\n",
    "                        filtered_entries.append(\">\" + entry)\n",
    "                    else:\n",
    "                        filtered_entries_count[filename] += 1\n",
    "                filtered_content = \"\".join(filtered_entries)\n",
    "                with open(output_file, 'a') as outfile:\n",
    "                    outfile.write(filtered_content)\n",
    "                species_char_count[filename] += len(filtered_content)\n",
    "        else:\n",
    "            tqdm.write(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "            \n",
    "        time.sleep(random.uniform(0.5, 1.5))  # delay\n",
    "    \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error processing row {index}: {e}\")\n",
    "\n",
    "# Summary\n",
    "for species, count in species_char_count.items():\n",
    "    filtered = filtered_entries_count[species]\n",
    "    print(f\"Created {species}.fasta with {count} characters (coarse grained filtered out {filtered} mRNA entries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e69bd",
   "metadata": {},
   "source": [
    "### Filter out coding RNA\n",
    "Filtering out coding RNA is done using MMseqs2 against the Swiss-Prot database\n",
    "\n",
    "For easy of use and reproduceability this runs on BioLib as seen below.\n",
    "\n",
    "This is run on a preproccesed data-record: https://biolib.com/ncRNA-foundational-model/MAM-PRI-ROD-TSA/ for 'PRI|MAM|ROD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import biolib\n",
    "biolib.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmseqs2_code_filter = biolib.load('ncRNA-foundational-model/blast-species') # Right now this runs on a data-record containing all raw 'PRI|MAM|ROD' TSA records (created above). App is not able to take input right now.\n",
    "# Starting multple jobs for faster computation\n",
    "# Determine the range and splits based on which species families used.\n",
    "job_0_4 = mmseqs2_code_filter.cli(args=['--range', '0-4'], blocking=False)\n",
    "job_5_9 = mmseqs2_code_filter.cli(args=['--range', '5-9'], blocking=False)\n",
    "job_12_19 = mmseqs2_code_filter.cli(args=['--range', '12-19'], blocking=False)\n",
    "job_20_29 = mmseqs2_code_filter.cli(args=['--range', '20-29'], blocking=False)\n",
    "job_30_39 = mmseqs2_code_filter.cli(args=['--range', '30-39'], blocking=False)\n",
    "job_44_49 = mmseqs2_code_filter.cli(args=['--range', '44-49'], blocking=False)\n",
    "job_50_59 = mmseqs2_code_filter.cli(args=['--range', '50-59'], blocking=False)\n",
    "job_60_66 = mmseqs2_code_filter.cli(args=['--range', '60-66'], blocking=False)\n",
    "\n",
    "jobs =[job_0_4, job_5_9, job_12_19, job_20_29, job_30_39, job_44_49, job_50_59, job_60_66]\n",
    "for job in jobs:\n",
    "    job.wait() # wait for finish\n",
    "    job.save_files(output_dir='tsa_species_fasta_non_coding_mmseqs2/', path_filter='*/*.fasta') # download all results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa81eaf",
   "metadata": {},
   "source": [
    "### GC and entropy filtering\n",
    "\n",
    "As an additional filtering step, sequences are retained only if they meet the following criteria:\n",
    "- GC content is between 30% and 80%.\n",
    "- Sequence length is at least 10 nucleotides.\n",
    "- Shannon entropy is at least 1.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c32800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "MIN_GC = 0.3\n",
    "MAX_GC = 0.8\n",
    "MIN_ENTROPY = 1.75\n",
    "MIN_LEN = 10\n",
    "\n",
    "def shannon_entropy(seq):\n",
    "    \"\"\"Calculates the Shannon entropy of a sequence.\"\"\"\n",
    "    freq = {}\n",
    "    for base in seq:\n",
    "        # Consider only standard DNA/RNA bases\n",
    "        if base in \"ACGTU\":\n",
    "            freq[base] = freq.get(base, 0) + 1\n",
    "    total = sum(freq.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "\n",
    "    entropy = -sum((count / total) * math.log2(count / total)\n",
    "                   for count in freq.values() if count > 0)\n",
    "    return entropy\n",
    "\n",
    "def gc_content(seq):\n",
    "    seq = seq.upper()\n",
    "    g = seq.count('G')\n",
    "    c = seq.count('C')\n",
    "    length = len(seq)\n",
    "    return (g + c) / length if length > 0 else 0\n",
    "\n",
    "def process_fasta_file(input_filepath, output_filepath):\n",
    "    filtered_records = []\n",
    "    total_sequences = 0\n",
    "    removed_sequences = 0\n",
    "\n",
    "    print(f\"Processing {os.path.basename(input_filepath)}...\")\n",
    "    with open(input_filepath, \"r\") as handle:\n",
    "        initial_count = sum(1 for _ in SeqIO.parse(handle, \"fasta\"))\n",
    "    \n",
    "    with open(input_filepath, \"r\") as handle:\n",
    "        for record in tqdm(SeqIO.parse(handle, \"fasta\"), total=initial_count, unit=\"seq\"):\n",
    "            total_sequences += 1\n",
    "            seq = str(record.seq).upper().replace(\"N\", \"\")\n",
    "            seq_len = len(seq)\n",
    "\n",
    "            if seq_len < MIN_LEN:\n",
    "                removed_sequences += 1\n",
    "                continue\n",
    "\n",
    "            gc = gc_content(seq)\n",
    "            ent = shannon_entropy(seq)\n",
    "\n",
    "            if MIN_GC <= gc <= MAX_GC and ent >= MIN_ENTROPY:\n",
    "                filtered_records.append(record)\n",
    "            else:\n",
    "                removed_sequences += 1\n",
    "\n",
    "    if filtered_records:\n",
    "        SeqIO.write(filtered_records, output_filepath, \"fasta\")\n",
    "\n",
    "    return removed_sequences, total_sequences\n",
    "\n",
    "\n",
    "def main():\n",
    "    MIN_GC = 0.3\n",
    "    MAX_GC = 0.8\n",
    "    MIN_ENTROPY = 1.0\n",
    "    MIN_LEN = 10\n",
    "    DEFAULT_OUTPUT_DIR = \"non_coding_filtered_entropy\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_dir\",)\n",
    "    parser.add_argument(\"--output_dir\", default=DEFAULT_OUTPUT_DIR)\n",
    "    parser.add_argument(\"--min_gc\", type=float, default=MIN_GC)\n",
    "    parser.add_argument(\"--max_gc\", type=float, default=MAX_GC)\n",
    "    parser.add_argument(\"--min_entropy\", type=float, default=MIN_ENTROPY)\n",
    "    parser.add_argument(\"--min_len\", type=int, default=MIN_LEN)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    input_dir = args.input_dir\n",
    "    output_dir = args.output_dir\n",
    "\n",
    "    total_removed_sequences = 0\n",
    "    total_sequences_processed = 0\n",
    "\n",
    "    fasta_files_found = False\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith(\".fasta\")\n",
    "            fasta_files_found = True\n",
    "            input_filepath = os.path.join(input_dir, filename)\n",
    "            output_filepath = os.path.join(output_dir, filename)\n",
    "            removed, processed = process_fasta_file(input_filepath, output_filepath)\n",
    "            total_removed_sequences += removed\n",
    "            total_sequences_processed += processed\n",
    "\n",
    "    print(f\"Total sequences filtered out from all files: {total_removed_sequences}\")\n",
    "    print(f\"Total sequences processed from all files: {total_sequences_processed}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04145acf",
   "metadata": {},
   "source": [
    "### Homology filtering\n",
    "Homology filtering is done using MMseqs2\n",
    "\n",
    "This will generate the m8 files with all inter species homology hits as well as sequence identities.\n",
    "\n",
    "This also runs on a precomputed data-record: https://biolib.com/ncRNA-foundational-model/non-coding-filtered-MAM-PRI-ROD-TSA/ for 'PRI|MAM|ROD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_species = biolib.load('ncRNA-foundational-model/homology-species')\n",
    "\n",
    "job_0_14 = homology_species.cli(args=[\n",
    "    '--range', '0-14'\n",
    "], blocking=False)\n",
    "\n",
    "job_15_29 = homology_species.cli(args=[\n",
    "    '--range', '15-29'\n",
    "], blocking=False)\n",
    "\n",
    "job_30_44 = homology_species.cli(args=[\n",
    "    '--range', '30-44'\n",
    "], blocking=False)\n",
    "\n",
    "job_45_62 = homology_species.cli(args=[\n",
    "    '--range', '45-62'\n",
    "], blocking=False)\n",
    "\n",
    "jobs = [job_0_14, job_15_29, job_30_44, job_45_62]\n",
    "for job in jobs:\n",
    "    job.wait() # wait for finish\n",
    "    job.save_files(output_dir=f\"tsa_species_fasta_m8_files\", path_filter=\"tmp_search/*_results.m8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d688fb",
   "metadata": {},
   "source": [
    "The m8 files are used with the non-coding FASTAs to filter based on the following thresholds:\n",
    "\n",
    "| Sequence Identity | Minimum Inter-Species Hits |\n",
    "| :---------------- | :------------------------- |\n",
    "| 75%               | 1                          |\n",
    "| 75%               | 3                          |\n",
    "| 75%               | 5                          |\n",
    "| 75%               | 7                          |\n",
    "| 80%               | 1                          |\n",
    "| 80%               | 3                          |\n",
    "| 80%               | 5                          |\n",
    "| 80%               | 7                          |\n",
    "| 85%               | 1                          |\n",
    "| 85%               | 3                          |\n",
    "| 85%               | 5                          |\n",
    "| 85%               | 7                          |\n",
    "| 90%               | 1                          |\n",
    "| 90%               | 3                          |\n",
    "| 90%               | 5                          |\n",
    "| 90%               | 7                          |\n",
    "| 95%               | 1                          |\n",
    "| 95%               | 3                          |\n",
    "| 95%               | 5                          |\n",
    "| 95%               | 7                          |\n",
    "| 99%               | 1                          |\n",
    "| 99%               | 3                          |\n",
    "| 99%               | 5                          |\n",
    "| 99%               | 7                          |\n",
    "\n",
    "This can be done locally by running the below code. Adjust paths as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from Bio.Seq import Seq\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "M8_DIR = \"m8_files/tmp_search\"\n",
    "FASTA_INPUT_DIR = \"non_coding_filtered_entropy\"\n",
    "BASE_OUTPUT_DIR = \"conserved_rnas_filtered_flanked\"\n",
    "QUERY_CONSERVATION_CUTOFFS = [0.0, 0.50]\n",
    "THRESHOLD_COMBINATIONS = [\n",
    "    (0.99, 7), (0.99, 5), (0.99, 3), (0.99, 1),\n",
    "    (0.95, 7), (0.95, 5), (0.95, 3), (0.95, 1),\n",
    "    (0.90, 7), (0.90, 5), (0.90, 3), (0.90, 1),\n",
    "    (0.85, 7), (0.85, 5), (0.85, 3), (0.85, 1),\n",
    "    (0.80, 7), (0.80, 5), (0.80, 3), (0.80, 1),\n",
    "    (0.75, 7), (0.75, 5), (0.75, 3), (0.75, 1),\n",
    "]\n",
    "MIN_ALIGN_LEN = 20\n",
    "EVALUE_THRESHOLD = 1e-10\n",
    "flank=20\n",
    "\n",
    "def parse_fasta_stream(fasta_path):\n",
    "    seqs = {}\n",
    "    with open(fasta_path) as f:\n",
    "        seq_id = None\n",
    "        seq_lines = []\n",
    "        for line in tqdm(f, desc=f\"Parsing FASTA: {os.path.basename(fasta_path)}\", leave=False):\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\">\"):\n",
    "                if seq_id:\n",
    "                    seqs[seq_id] = \"\\n\".join(seq_lines)\n",
    "                seq_id = line[1:].split()[0]\n",
    "                seq_lines = [line]\n",
    "            else:\n",
    "                seq_lines.append(line)\n",
    "        if seq_id:\n",
    "            seqs[seq_id] = \"\\n\".join(seq_lines)\n",
    "    return seqs\n",
    "\n",
    "def parse_m8_stream(m8_path, species):\n",
    "    best_hits = {}\n",
    "    alignment_coords = {}\n",
    "    with open(m8_path) as f:\n",
    "        for line in tqdm(f, desc=f\"Parsing M8: {os.path.basename(m8_path)}\", leave=False):\n",
    "            parts = line.rstrip().split(\"\\t\")\n",
    "            if len(parts) < 14:\n",
    "                continue\n",
    "            query, target = parts[0], parts[1]\n",
    "            identity = float(parts[2])\n",
    "            align_len = int(parts[3])\n",
    "            evalue = float(parts[10])\n",
    "            qcov = float(parts[12])\n",
    "            hit_species = target.split(\"__\")[0]\n",
    "            if hit_species == species:\n",
    "                continue\n",
    "            key = (query, hit_species)\n",
    "            if key not in best_hits or identity > best_hits[key][2] or (identity == best_hits[key][2] and qcov > best_hits[key][4]):\n",
    "                best_hits[key] = (query, target, identity, align_len, qcov, evalue)\n",
    "                alignment_coords[key] = (int(parts[6]), int(parts[7]))\n",
    "    return best_hits, alignment_coords\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--flank-only\", action=\"store_true\", help=\"Only write flanked alignment dataset, skip full sequences\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    fasta_dir = os.path.join(os.path.dirname(__file__), FASTA_INPUT_DIR)\n",
    "    m8_dir = M8_DIR\n",
    "    base_output_dir = os.path.join(os.path.dirname(__file__), BASE_OUTPUT_DIR)\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "    fasta_files = [f for f in os.listdir(fasta_dir) if f.endswith(\".fasta\")]\n",
    "    all_species = [os.path.splitext(f)[0].replace(\"_non_coding\", \"\") for f in fasta_files]\n",
    "    total_possible_species = len(all_species) - 1\n",
    "\n",
    "    # Prepare output writers for each combination\n",
    "    output_handles = {}\n",
    "    stats_handles = {}\n",
    "    alignment_handles = {}\n",
    "    for identity, min_species in THRESHOLD_COMBINATIONS:\n",
    "        for qcons in QUERY_CONSERVATION_CUTOFFS:\n",
    "            outdir = os.path.join(base_output_dir, f\"run_{identity}_{min_species}-sp_querycons{qcons}\")\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            fasta_path = os.path.join(outdir, \"conserved_sequences.fasta\")\n",
    "            stats_path = os.path.join(outdir, \"stats.tsv\")\n",
    "            alignment_path = os.path.join(outdir, \"conserved_alignment.fasta\")\n",
    "            output_handles[(identity, min_species, qcons)] = open(fasta_path, \"w\")\n",
    "            stats_handles[(identity, min_species, qcons)] = open(stats_path, \"w\", newline='')\n",
    "            alignment_handles[(identity, min_species, qcons)] = open(alignment_path, \"w\")\n",
    "            stats_writer = csv.writer(stats_handles[(identity, min_species, qcons)], delimiter='\\t')\n",
    "            stats_writer.writerow([\"species\", \"unique_species_hit\", \"conserved_rna_count\"])\n",
    "\n",
    "    print(f\"Processing {len(fasta_files)} species...\")\n",
    "    for fasta_file in tqdm(fasta_files, desc=\"Species\", unit=\"species\"):\n",
    "        species = os.path.splitext(fasta_file)[0].replace(\"_non_coding\", \"\")\n",
    "        fasta_path = os.path.join(fasta_dir, fasta_file)\n",
    "        m8_path = os.path.join(m8_dir, f\"{species}_results.m8\")\n",
    "        if not os.path.exists(m8_path):\n",
    "            print(f\"Warning: {m8_path} not found, skipping {species}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing species: {species}\")\n",
    "        seqs = parse_fasta_stream(fasta_path)\n",
    "        best_hits, alignment_coords = parse_m8_stream(m8_path, species)\n",
    "\n",
    "        for identity, min_species in tqdm(THRESHOLD_COMBINATIONS, desc=f\"Thresholds for {species}\", leave=False):\n",
    "            query2species = defaultdict(set)\n",
    "            query2hits = defaultdict(list)\n",
    "            for (query, hitsp), (q, t, ident, alen, qcov, evalue) in best_hits.items():\n",
    "                if ident >= identity and alen >= MIN_ALIGN_LEN and evalue <= EVALUE_THRESHOLD:\n",
    "                    query2species[query].add(hitsp)\n",
    "                    query2hits[query].append((hitsp, q, t, ident, alen, qcov, evalue))\n",
    "            filtered_queries = {q for q, sps in query2species.items() if len(sps) >= min_species}\n",
    "\n",
    "            for qcons in QUERY_CONSERVATION_CUTOFFS:\n",
    "                out_handle = output_handles[(identity, min_species, qcons)]\n",
    "                stats_writer = csv.writer(stats_handles[(identity, min_species, qcons)], delimiter='\\t')\n",
    "                alignment_handle = alignment_handles[(identity, min_species, qcons)]\n",
    "                conserved_ids = []\n",
    "                for q in filtered_queries:\n",
    "                    n_species = len(query2species[q])\n",
    "                    if total_possible_species > 0 and (n_species / total_possible_species) >= qcons:\n",
    "                        conserved_ids.append(q)\n",
    "                # Write sequences\n",
    "                for q in conserved_ids:\n",
    "                    if q in seqs:\n",
    "                        lines = seqs[q].split(\"\\n\")\n",
    "                        lines[0] = f\">{species}__{lines[0][1:]}\"\n",
    "                        if not args.flank_only:\n",
    "                            out_handle.write(\"\\n\".join(lines) + \"\\n\")\n",
    "                        for hit in query2hits[q]:\n",
    "                            hitsp, qid, tid, ident, alen, qcov, evalue = hit\n",
    "                            qstart, qend = alignment_coords.get((q, hitsp), (None, None))\n",
    "                            if qstart is None or qend is None:\n",
    "                                print(f\"Warning: No alignment coords for {q}, {hitsp}\")\n",
    "                                continue\n",
    "                            seq = ''.join([l for l in seqs[q].split(\"\\n\")[1:]])\n",
    "                            start = max(0, min(qstart, qend) - 1 - flank)\n",
    "                            end = min(len(seq), max(qstart, qend) + flank)\n",
    "                            subseq = seq[start:end]\n",
    "                            if qstart > qend:\n",
    "                                subseq = str(Seq(subseq).reverse_complement())\n",
    "                            header = f\">{species}__{q}__{hitsp}__{qstart}_{qend}_flank{flank}\"\n",
    "                            if subseq:\n",
    "                                alignment_handle.write(f\"{header}\\n{subseq}\\n\")\n",
    "                            else:\n",
    "                                print(f\"Warning: Empty alignment for {header}\")\n",
    "                unique_species_hit = len({sp for q in conserved_ids for sp in query2species[q]})\n",
    "                stats_writer.writerow([species, unique_species_hit, len(conserved_ids)])\n",
    "                print(f\"  [identity={identity}, min_species={min_species}, qcons={qcons}] {species}: {len(conserved_ids)} conserved, {unique_species_hit} unique species hit\")\n",
    "\n",
    "    # Close all output files\n",
    "    for handle in output_handles.values():\n",
    "        handle.close()\n",
    "    for handle in stats_handles.values():\n",
    "        handle.close()\n",
    "    for handle in alignment_handles.values():\n",
    "        handle.close()\n",
    "\n",
    "    print(\"All threshold and query conservation combinations processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2321e",
   "metadata": {},
   "source": [
    "### Redundancy clustering\n",
    "Redudancy clustering is done using MMSEQS2 with 90% identity and 80% coverage\n",
    "\n",
    "This will generate and output datasets with less redundancy since similar sequences are removed.\n",
    "\n",
    "This is done one a precomputed data-record: https://biolib.com/ncRNA-foundational-model/TSA-conserved-before-clustering/ for 'PRI|MAM|ROD' and run through a BioLib application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmseqs_clustering = biolib.load('ncRNA-foundational-model/mmseqs-clustering')\n",
    "\n",
    "# Running without arguments since data-record is preloaded\n",
    "job = mmseqs_clustering.run()\n",
    "job.wait() # wait for finish\n",
    "job.save_files(output_dir='final_datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f2326",
   "metadata": {},
   "source": [
    "### Final dataset(s)\n",
    "The final dataset(s) can now be found in `final_datasets`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
